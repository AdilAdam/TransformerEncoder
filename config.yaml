encoder: "transformer"
encoder_conf:
    output_size: 512    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    use_cnn_module: True
    activation_type: 'Sigmoid'

grad_clip: 5
accum_grad: 4
max_epoch: 10
log_interval: 100

optim: adam
optim_conf:
    lr: 0.002
scheduler: warmuplr    
scheduler_conf:
    warmup_steps: 25000