![Transformer-encoder](https://github.com/AdilAdam/TransformerEncoder/assets/126153483/120b0585-20ee-4273-b714-b9d909546b6c)

<div align="center">

**PyTorch implementation of Transformer encoder: Transformer encoder for Speech Recognition.**

  
</div>

***
<p  align="center"> 
     <a href="https://github.com/sooftware/jasper/blob/main/LICENSE">
          <img src="http://img.shields.io/badge/license-Apache--2.0-informational"> 
     </a>
     <a href="https://github.com/pytorch/pytorch">
          <img src="http://img.shields.io/badge/framework-PyTorch-informational"> 
     </a>
     <a href="https://www.python.org/dev/peps/pep-0008/">
          <img src="http://img.shields.io/badge/codestyle-PEP--8-informational"> 
     </a>
     <a href="https://github.com/sooftware/conformer">
          <img src="http://img.shields.io/badge/build-passing-success"> 
     </a>
     <a href="https://sooftware.github.io/KoSpeech/Conformer.html">
          <img src="http://img.shields.io/badge/docs-passing-success"> 
     </a>
  
Transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence, which means Transformer models are good at capturing content-based global interactions.Transformer models apply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant data elements in a series influence and depend on each other.
![transformer](https://github.com/AdilAdam/TransformerEncoder/assets/126153483/d9385695-81e8-4cec-98bd-d0939744e049)


In this repository just transformer encoder was implemented, for more detail further check the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

## TODO
- Transformer encoder implementation for learning or research purpose.
- Implemented on Pytorch (version=1.11.0) environment, Python vesrion 3.8.16
